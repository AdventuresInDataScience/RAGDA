Metadata-Version: 2.4
Name: ragda
Version: 2.1.0
Summary: RAGDA - High-Performance Derivative-Free Optimizer with High-Dimensional Support
Home-page: https://github.com/AdventuresInDataScience/RAGDA
Author: RAGDA Team
Author-email: 
License-Expression: MIT
Project-URL: Homepage, https://github.com/AdventuresInDataScience/RAGDA
Project-URL: Repository, https://github.com/AdventuresInDataScience/RAGDA
Project-URL: Issues, https://github.com/AdventuresInDataScience/RAGDA/issues
Keywords: optimization,derivative-free,hyperparameter-tuning,machine-learning,cython
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Developers
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Cython
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Scientific/Engineering :: Mathematics
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: numpy>=1.20.0
Requires-Dist: pandas>=1.3.0
Requires-Dist: scipy>=1.7.0
Requires-Dist: loky>=3.0.0
Provides-Extra: dev
Requires-Dist: cython>=0.29.0; extra == "dev"
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: matplotlib>=3.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx; extra == "docs"
Requires-Dist: sphinx-rtd-theme; extra == "docs"
Dynamic: home-page
Dynamic: requires-python

# RAGDA - Robust ADAM Gradient Descent Approximator

A **pure Cython/C** derivative-free optimization library for hyperparameter tuning and black-box optimization. RAGDA uses ADAM-based gradient approximation with parallel workers for fast, robust optimization.

## Features

- ðŸš€ **Pure Cython/C** - No Python fallbacks, maximum performance
- âš¡ **Parallel optimization** - Multiple workers with different exploration strategies  
- ðŸŽ¯ **Mixed variable support** - Continuous, categorical, and ordinal parameters
- ðŸ“Š **Mini-batch curriculum learning** - Start with small batches, scale up
- ðŸ”„ **Adaptive sigma shrinking** - Automatic step size reduction on plateaus
- ðŸ›‘ **Early stopping** - Convergence detection per worker
- ðŸ”— **Worker synchronization** - Share best solutions across workers
- ðŸ“ˆ **ADAM optimizer** - Momentum-based gradient approximation
- ðŸ”¬ **High-dimensional optimization** - Kernel PCA, random projections, and adaptive dimensionality reduction

## Installation

### Using pip
```bash
pip install ragda
```

### From source
```bash
git clone https://github.com/AdventuresInDataScience/ragda.git
cd ragda
pip install .
```

### Requirements

RAGDA automatically installs these dependencies:
- `numpy>=1.20.0` - Core numerical operations
- `pandas>=1.3.0` - Results DataFrames and analysis
- `scipy>=1.7.0` - Latin Hypercube Sampling for initialization
- `cython>=3.0.0` - Build-time dependency for compilation
- `loky>=3.0.0` - Robust parallel execution (Windows-safe)

**Platform Support:** Linux âœ“ | macOS âœ“ | Windows âœ“

**Python Support:** Python 3.8+

## Quick Start

### Basic Optimization

```python
from ragda import RAGDAOptimizer
import numpy as np

# Define search space
space = [
    {'name': 'learning_rate', 'type': 'continuous', 'bounds': [1e-5, 1e-1]},
    {'name': 'dropout', 'type': 'continuous', 'bounds': [0.0, 0.5]},
    {'name': 'optimizer', 'type': 'categorical', 'values': ['adam', 'sgd', 'rmsprop']},
    {'name': 'layers', 'type': 'ordinal', 'values': [1, 2, 3, 4, 5]},
]

# Define objective function
def objective(params):
    # Your model training/evaluation code here
    # Return the metric to minimize (e.g., validation loss)
    return some_loss

# Optimize
optimizer = RAGDAOptimizer(space, n_workers=4, random_state=42)
result = optimizer.optimize(objective, n_trials=100)

print(f"Best params: {result.best_params}")
print(f"Best value: {result.best_value}")
```

### Scipy-Style Interface

For simple continuous optimization:

```python
from ragda import ragda_optimize
import numpy as np

def rosenbrock(x):
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

bounds = np.array([[-2, 2], [-2, 2]], dtype=np.float64)
x_best, f_best, info = ragda_optimize(rosenbrock, bounds, n_trials=100)

print(f"Best x: {x_best}")
print(f"Best f: {f_best}")
```

### High-Dimensional Optimization

For problems with 100+ dimensions, RAGDA provides specialized tools that exploit low-dimensional structure:

```python
from ragda import HighDimRAGDAOptimizer
import numpy as np

# Define a 500-dimensional search space
n_dims = 500
space = [
    {'name': f'x{i}', 'type': 'continuous', 'bounds': [-5.0, 5.0]}
    for i in range(n_dims)
]

def high_dim_objective(params):
    # Only first 10 dimensions matter (common in practice)
    return sum((params[f'x{i}'] - 1.0)**2 for i in range(10))

optimizer = HighDimRAGDAOptimizer(
    space,
    direction='minimize',
    dim_threshold=100,          # Use high-dim methods for 100+ dimensions
    variance_threshold=0.95,    # Capture 95% of variance
    reduction_method='auto',    # Auto-select: kernel_pca, incremental_pca, or random_projection
    n_workers=4,
    random_state=42
)

result = optimizer.optimize(
    high_dim_objective,
    n_trials=200,
    initial_samples=150,        # Samples for dimensionality analysis
    stage2_trials_fraction=0.2, # Fraction for trust-region refinement
)

print(f"Best value: {result.best_value}")
```

#### Scipy-Style High-Dim Interface

```python
from ragda import highdim_ragda_optimize
import numpy as np

def sphere(x):
    return np.sum(x**2)

bounds = np.array([[-5, 5]] * 200, dtype=np.float64)  # 200 dimensions

x_best, f_best, info = highdim_ragda_optimize(
    sphere,
    bounds,
    n_trials=200,
    reduction_method='kernel_pca',
    variance_threshold=0.90,
    verbose=True
)
```

#### Using the DimensionalityReducer Directly

```python
from ragda import DimensionalityReducer
import numpy as np

# Fit a reducer on your data
X = np.random.randn(1000, 500).astype(np.float64)

reducer = DimensionalityReducer(
    method='kernel_pca',  # or 'incremental_pca', 'random_projection'
    n_components=50,
    kernel='rbf',
    random_seed=42
)

reducer.fit(X)
X_reduced = reducer.transform(X)           # 1000 x 50
X_reconstructed = reducer.inverse_transform(X_reduced)  # 1000 x 500
```

### Mini-Batch Optimization (ML Training)

For expensive objectives with dataset subsampling:

```python
from ragda import RAGDAOptimizer
import numpy as np

# Your training data
X_train, y_train = load_data()  # 10,000 samples

def ml_objective(params, batch_size=-1):
    """
    When batch_size > 0, evaluate on a random subset.
    When batch_size = -1, evaluate on full dataset.
    """
    if batch_size > 0 and batch_size < len(X_train):
        idx = np.random.choice(len(X_train), batch_size, replace=False)
        X, y = X_train[idx], y_train[idx]
    else:
        X, y = X_train, y_train
    
    # Train and evaluate model
    model = train_model(X, y, **params)
    return evaluate_model(model, X, y)

space = [
    {'name': 'learning_rate', 'type': 'continuous', 'bounds': [1e-5, 1e-1]},
    {'name': 'weight_decay', 'type': 'continuous', 'bounds': [1e-6, 1e-2]},
]

optimizer = RAGDAOptimizer(space, n_workers=4, random_state=42)
result = optimizer.optimize(
    ml_objective,
    n_trials=100,
    use_minibatch=True,
    data_size=10000,           # Total dataset size
    minibatch_start=100,       # Start with 100 samples
    minibatch_end=5000,        # End with 5000 samples
    minibatch_schedule='inverse_decay',  # Curriculum schedule
)

# Final result is re-evaluated on full dataset
print(f"Best params: {result.best_params}")
print(f"Best value: {result.best_value}")
```

## API Reference

### RAGDAOptimizer

```python
RAGDAOptimizer(
    space: List[Dict],              # Search space definition
    direction: str = 'minimize',    # 'minimize' or 'maximize'
    n_workers: int = None,          # Number of parallel workers (default: CPU count)
    random_state: int = None,       # Random seed for reproducibility
)
```

### optimize()

```python
result = optimizer.optimize(
    objective: Callable,           # Function to optimize
    n_trials: int = 1000,          # Iterations per worker
    x0: Dict = None,               # Starting point
    verbose: bool = True,          # Print progress
    
    # Early stopping
    early_stop_threshold: float = 1e-12,
    early_stop_patience: int = 50,
    
    # Sigma shrinking (adaptive step size)
    shrink_factor: float = 0.9,
    shrink_patience: int = 10,
    shrink_threshold: float = 1e-6,
    
    # Mini-batch settings
    use_minibatch: bool = False,
    data_size: int = 1000,
    minibatch_start: int = 50,
    minibatch_end: int = 500,
    minibatch_schedule: str = 'linear',  # 'linear', 'exponential', 'inverse_decay'
    
    # Worker synchronization
    sync_frequency: int = 100,     # Share best solution every N iters
)
```

### OptimizationResult

```python
result.best_params    # Dict of best parameters
result.best_value     # Best objective value
result.n_trials       # Number of iterations
result.n_workers      # Number of workers used
result.direction      # 'minimize' or 'maximize'
```

### HighDimRAGDAOptimizer

For high-dimensional problems (100+ dimensions):

```python
HighDimRAGDAOptimizer(
    space: List[Dict],                  # Search space definition
    direction: str = 'minimize',        # 'minimize' or 'maximize'
    dim_threshold: int = 100,           # Use high-dim methods above this
    variance_threshold: float = 0.95,   # Variance to capture (0.7-0.99)
    reduction_method: str = 'auto',     # 'auto', 'kernel_pca', 'incremental_pca', 'random_projection'
    trust_region_fraction: float = 0.1, # Trust region size for refinement
    stage2_trials_fraction: float = 0.2,# Fraction of trials for stage 2
    n_workers: int = None,              # Number of parallel workers
    random_state: int = None,           # Random seed
    initial_samples: int = 100,         # Samples for dimensionality analysis
)
```

**Reduction Methods:**
- `kernel_pca`: RBF Kernel PCA for nonlinear structure (best for smooth objectives)
- `incremental_pca`: Online PCA for streaming/large datasets
- `random_projection`: Johnson-Lindenstrauss random projections (fastest)
- `auto`: Automatically selects based on problem characteristics

## Search Space Definition

### Continuous Parameters
```python
{'name': 'param_name', 'type': 'continuous', 'bounds': [lower, upper]}
```

### Categorical Parameters
```python
{'name': 'param_name', 'type': 'categorical', 'values': ['a', 'b', 'c']}
```

### Ordinal Parameters
```python
{'name': 'param_name', 'type': 'ordinal', 'values': [1, 2, 3, 4, 5]}
```

## How It Works

RAGDA uses a **derivative-free gradient approximation** approach:

1. **Sampling**: Generate candidate solutions around the current best
2. **Evaluation**: Evaluate candidates (optionally in parallel batches)
3. **Gradient Estimation**: Compute approximate gradient from improvement directions
4. **ADAM Update**: Apply ADAM optimizer with momentum
5. **Shrinking**: Reduce step size when stuck on plateaus

### Multi-Worker Strategy

Each worker uses a different `top_n` fraction (exploration vs exploitation):
- Worker 0: top_n=100% (full exploration)
- Worker 1: top_n=89%
- ...
- Worker N: top_n=20% (greedy exploitation)

Workers periodically synchronize, sharing the global best solution.

### High-Dimensional Optimization Strategy

For problems with 100+ dimensions, RAGDA uses a **two-stage approach**:

1. **Dimensionality Analysis**: Sample the search space and analyze eigenvalue spectrum
2. **Effective Dimension Detection**: Identify if low-dimensional structure exists
3. **Stage 1 - Reduced Space**: Optimize in the low-dimensional subspace found by Kernel PCA
4. **Stage 2 - Trust Region**: Refine the solution in the full space around the Stage 1 result

**Key algorithms (all pure C/Cython for speed):**
- Kernel PCA with RBF kernel and adaptive gamma (median heuristic)
- Quickselect-based O(n) median computation for gamma estimation
- Johnson-Lindenstrauss random projections (Gaussian, sparse, Rademacher)
- Incremental PCA for online/streaming updates

## Running Tests

```bash
# Quick validation tests
python test_quick.py

# Full test suite
python -m pytest tests/ -v

# Exclude slow tests
python -m pytest tests/ -v -m "not slow"
```

## Performance Tips

1. **Use more workers** for hard multi-modal problems
2. **Enable mini-batch** for expensive objectives with large datasets
3. **Tune shrink_patience** - lower for fast convergence, higher for exploration
4. **Set sync_frequency** - more frequent syncing helps on unimodal problems
5. **For high-dim problems** - use `HighDimRAGDAOptimizer` when dimensions > 100
6. **Choose reduction method** - `kernel_pca` for smooth objectives, `random_projection` for speed

## Verifying Installation

```python
import ragda
print(f"RAGDA version: {ragda.__version__}")

# Quick verification
from ragda import ragda_optimize
import numpy as np

def sphere(x):
    return np.sum(x**2)

bounds = np.array([[-5, 5]] * 3, dtype=np.float64)
x_best, f_best, info = ragda_optimize(sphere, bounds, n_trials=50)
print(f"Test optimization: f_best = {f_best:.6f} (should be near 0)")

# Check high-dim module
if ragda.HIGHDIM_AVAILABLE:
    print("High-dimensional optimization: Available âœ“")
else:
    print("High-dimensional optimization: Not built (run pip install -e .)")
```

## Changelog

### v2.1.0
- **High-dimensional optimization** with Kernel PCA, random projections, and incremental PCA
- Two-stage optimization: reduced space exploration + trust region refinement
- Adaptive variance threshold that increases during optimization
- Pure C implementations for all distance/kernel computations (no scipy dependency in hot paths)
- Quickselect-based O(n) median for RBF kernel gamma estimation
- `HighDimRAGDAOptimizer` class and `highdim_ragda_optimize` convenience function
- `DimensionalityReducer` wrapper for standalone dimensionality reduction
- 31 new tests for high-dimensional functionality

### v2.0.0
- Pure Cython/C implementation (no Python fallbacks)
- Fixed memory corruption issues with preallocated NumPy arrays for history tracking
- Added final full-sample re-evaluation for mini-batch optimization
- Comprehensive test suite (65+ unit and integration tests)
- Windows support via loky for multiprocessing

### v1.0.0
- Initial release

## License

MIT License - see LICENSE file for details.

## Citation

If you use RAGDA in your research, please cite:

```bibtex
@software{ragda2024,
  title = {RAGDA: Robust ADAM Gradient Descent Approximator},
  author = {Adventures In Data Science},
  year = {2024},
  url = {https://github.com/AdventuresInDataScience/ragda}
}
```
